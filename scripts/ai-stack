#!/bin/bash
#
# ai-stack - Management script for Local AI Stack
# Controls Ollama and ComfyUI services on macOS with Apple Silicon
#

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
BOLD='\033[1m'
NC='\033[0m' # No Color

# Configuration
COMFYUI_DIR="${HOME}/local-ai-stack/ComfyUI"
COMFYUI_PORT=8188
OLLAMA_PORT=11434
COMFYUI_PIDFILE="${COMFYUI_DIR}/.comfyui.pid"

# Helper functions
print_header() {
    echo -e "\n${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo -e "${BOLD}${BLUE}  $1${NC}"
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}\n"
}

print_status() {
    local service=$1
    local status=$2
    local port=$3

    if [ "$status" = "running" ]; then
        echo -e "  ${GREEN}●${NC} ${BOLD}$service${NC} - ${GREEN}running${NC} (port $port)"
    else
        echo -e "  ${RED}●${NC} ${BOLD}$service${NC} - ${RED}stopped${NC}"
    fi
}

print_success() {
    echo -e "${GREEN}✓${NC} $1"
}

print_error() {
    echo -e "${RED}✗${NC} $1"
}

print_info() {
    echo -e "${CYAN}ℹ${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}⚠${NC} $1"
}

# Check if a service is running on a port
is_port_open() {
    local port=$1
    lsof -i :$port >/dev/null 2>&1
}

# Check if Ollama is running
is_ollama_running() {
    brew services list 2>/dev/null | grep -q "ollama.*started" || is_port_open $OLLAMA_PORT
}

# Check if ComfyUI is running
is_comfyui_running() {
    if [ -f "$COMFYUI_PIDFILE" ]; then
        local pid=$(cat "$COMFYUI_PIDFILE")
        if ps -p $pid > /dev/null 2>&1; then
            return 0
        fi
    fi
    # Fallback: check if port is in use
    is_port_open $COMFYUI_PORT
}

# Get ComfyUI PID
get_comfyui_pid() {
    if [ -f "$COMFYUI_PIDFILE" ]; then
        cat "$COMFYUI_PIDFILE"
    else
        lsof -ti :$COMFYUI_PORT 2>/dev/null | head -1
    fi
}

# Start Ollama
start_ollama() {
    if is_ollama_running; then
        print_info "Ollama is already running"
    else
        print_info "Starting Ollama..."
        brew services start ollama
        sleep 2
        if is_ollama_running; then
            print_success "Ollama started successfully"
        else
            print_error "Failed to start Ollama"
            return 1
        fi
    fi
}

# Stop Ollama
stop_ollama() {
    if is_ollama_running; then
        print_info "Stopping Ollama..."
        brew services stop ollama
        sleep 1
        print_success "Ollama stopped"
    else
        print_info "Ollama is not running"
    fi
}

# Start ComfyUI
start_comfyui() {
    if is_comfyui_running; then
        print_info "ComfyUI is already running"
    else
        print_info "Starting ComfyUI..."

        if [ ! -d "$COMFYUI_DIR" ]; then
            print_error "ComfyUI directory not found: $COMFYUI_DIR"
            return 1
        fi

        cd "$COMFYUI_DIR"

        # Activate venv and start ComfyUI
        if [ -f "venv/bin/activate" ]; then
            source venv/bin/activate
            nohup python main.py --listen 127.0.0.1 --port $COMFYUI_PORT > /dev/null 2>&1 &
            echo $! > "$COMFYUI_PIDFILE"
            deactivate 2>/dev/null || true
        else
            print_error "ComfyUI virtual environment not found"
            return 1
        fi

        # Wait for startup
        print_info "Waiting for ComfyUI to start..."
        for i in {1..30}; do
            if is_port_open $COMFYUI_PORT; then
                print_success "ComfyUI started successfully"
                print_info "Web UI: http://127.0.0.1:$COMFYUI_PORT"
                return 0
            fi
            sleep 1
        done

        print_error "ComfyUI failed to start within 30 seconds"
        return 1
    fi
}

# Stop ComfyUI
stop_comfyui() {
    if is_comfyui_running; then
        print_info "Stopping ComfyUI..."
        local pid=$(get_comfyui_pid)
        if [ -n "$pid" ]; then
            kill $pid 2>/dev/null || true
            sleep 1
            # Force kill if still running
            if ps -p $pid > /dev/null 2>&1; then
                kill -9 $pid 2>/dev/null || true
            fi
        fi
        rm -f "$COMFYUI_PIDFILE"
        print_success "ComfyUI stopped"
    else
        print_info "ComfyUI is not running"
    fi
}

# Start command
cmd_start() {
    local service=${1:-all}

    print_header "Starting Services"

    case $service in
        ollama)
            start_ollama
            ;;
        comfyui)
            start_comfyui
            ;;
        all)
            start_ollama
            start_comfyui
            ;;
        *)
            print_error "Unknown service: $service"
            echo "Usage: ai-stack start [ollama|comfyui|all]"
            exit 1
            ;;
    esac
}

# Stop command
cmd_stop() {
    local service=${1:-all}

    print_header "Stopping Services"

    case $service in
        ollama)
            stop_ollama
            ;;
        comfyui)
            stop_comfyui
            ;;
        all)
            stop_comfyui
            stop_ollama
            ;;
        *)
            print_error "Unknown service: $service"
            echo "Usage: ai-stack stop [ollama|comfyui|all]"
            exit 1
            ;;
    esac
}

# Status command
cmd_status() {
    print_header "AI Stack Status"

    echo -e "${BOLD}Services:${NC}"

    if is_ollama_running; then
        print_status "Ollama" "running" $OLLAMA_PORT
    else
        print_status "Ollama" "stopped" $OLLAMA_PORT
    fi

    if is_comfyui_running; then
        print_status "ComfyUI" "running" $COMFYUI_PORT
    else
        print_status "ComfyUI" "stopped" $COMFYUI_PORT
    fi

    # Memory usage
    echo -e "\n${BOLD}Memory Usage:${NC}"
    local mem_used=$(vm_stat | awk '/Pages active/ {active=$3} /Pages wired/ {wired=$3} END {printf "%.1f", (active+wired)*4096/1024/1024/1024}')
    local mem_total=$(sysctl -n hw.memsize | awk '{printf "%.0f", $1/1024/1024/1024}')
    echo -e "  System: ${CYAN}${mem_used}GB${NC} / ${mem_total}GB used"

    # Ollama memory
    if is_ollama_running; then
        local ollama_mem=$(ps -o rss= -p $(pgrep -f "ollama" | head -1) 2>/dev/null | awk '{printf "%.1f", $1/1024/1024}')
        if [ -n "$ollama_mem" ]; then
            echo -e "  Ollama: ${CYAN}${ollama_mem}GB${NC}"
        fi
    fi

    # Disk usage for models
    echo -e "\n${BOLD}Disk Usage:${NC}"
    if [ -d "$HOME/.ollama/models" ]; then
        local ollama_size=$(du -sh "$HOME/.ollama/models" 2>/dev/null | cut -f1)
        echo -e "  Ollama models: ${CYAN}${ollama_size}${NC}"
    fi
    if [ -d "$COMFYUI_DIR/models" ]; then
        local comfyui_size=$(du -sh "$COMFYUI_DIR/models" 2>/dev/null | cut -f1)
        echo -e "  ComfyUI models: ${CYAN}${comfyui_size}${NC}"
    fi

    # Installed models summary
    echo -e "\n${BOLD}Installed Models:${NC}"
    if is_ollama_running; then
        local model_count=$(ollama list 2>/dev/null | tail -n +2 | wc -l | tr -d ' ')
        echo -e "  Ollama: ${CYAN}${model_count}${NC} models"
    else
        echo -e "  Ollama: ${YELLOW}service not running${NC}"
    fi

    # ComfyUI checkpoints
    local checkpoint_count=$(find "$COMFYUI_DIR/models/checkpoints" -name "*.safetensors" -o -name "*.ckpt" 2>/dev/null | wc -l | tr -d ' ')
    echo -e "  ComfyUI checkpoints: ${CYAN}${checkpoint_count}${NC} models"

    local upscale_count=$(find "$COMFYUI_DIR/models/upscale_models" -name "*.pth" -o -name "*.safetensors" 2>/dev/null | wc -l | tr -d ' ')
    echo -e "  ComfyUI upscalers: ${CYAN}${upscale_count}${NC} models"
}

# Models command
cmd_models() {
    print_header "Ollama Models"

    if ! is_ollama_running; then
        print_error "Ollama is not running. Start it with: ai-stack start ollama"
        exit 1
    fi

    echo -e "${BOLD}Installed Models:${NC}\n"

    # Format the ollama list output nicely
    ollama list 2>/dev/null | while IFS= read -r line; do
        if [[ $line == NAME* ]]; then
            echo -e "${CYAN}$line${NC}"
        else
            echo "  $line"
        fi
    done

    echo -e "\n${BOLD}Popular Models to Install:${NC}"
    echo -e "  ${CYAN}ollama pull llama3.2:3b${NC}     # Fast general use (2GB)"
    echo -e "  ${CYAN}ollama pull llama3.1:8b${NC}     # Higher quality (4.7GB)"
    echo -e "  ${CYAN}ollama pull codellama:7b${NC}    # Code-focused (3.8GB)"
    echo -e "  ${CYAN}ollama pull mistral:7b${NC}      # Great for chat (4.1GB)"
    echo -e "  ${CYAN}ollama pull deepseek-r1:8b${NC}  # Reasoning model (4.9GB)"
}

# Test command
cmd_test() {
    print_header "Health Check"

    local all_passed=true

    # Test Ollama
    echo -e "${BOLD}Testing Ollama...${NC}"
    if is_ollama_running; then
        print_success "Ollama service is running"

        # Test API
        if curl -s http://127.0.0.1:$OLLAMA_PORT/api/tags > /dev/null 2>&1; then
            print_success "Ollama API responding"
        else
            print_error "Ollama API not responding"
            all_passed=false
        fi

        # Check for models
        local model_count=$(ollama list 2>/dev/null | tail -n +2 | wc -l | tr -d ' ')
        if [ "$model_count" -gt 0 ]; then
            print_success "Found $model_count Ollama model(s)"
        else
            print_warning "No Ollama models installed"
        fi
    else
        print_error "Ollama is not running"
        all_passed=false
    fi

    # Test ComfyUI
    echo -e "\n${BOLD}Testing ComfyUI...${NC}"
    if is_comfyui_running; then
        print_success "ComfyUI service is running"

        # Test API
        if curl -s http://127.0.0.1:$COMFYUI_PORT/system_stats > /dev/null 2>&1; then
            print_success "ComfyUI API responding"
        else
            print_warning "ComfyUI API not responding (may still be starting)"
        fi
    else
        print_error "ComfyUI is not running"
        all_passed=false
    fi

    # Check models
    echo -e "\n${BOLD}Checking ComfyUI Models...${NC}"

    local checkpoint_count=$(find "$COMFYUI_DIR/models/checkpoints" -name "*.safetensors" -o -name "*.ckpt" 2>/dev/null | wc -l | tr -d ' ')
    if [ "$checkpoint_count" -gt 0 ]; then
        print_success "Found $checkpoint_count checkpoint model(s)"
    else
        print_warning "No checkpoint models found in ComfyUI"
    fi

    local upscale_count=$(find "$COMFYUI_DIR/models/upscale_models" -name "*.pth" -o -name "*.safetensors" 2>/dev/null | wc -l | tr -d ' ')
    if [ "$upscale_count" -gt 0 ]; then
        print_success "Found $upscale_count upscale model(s)"
    else
        print_warning "No upscale models found in ComfyUI"
    fi

    # Summary
    echo ""
    if [ "$all_passed" = true ]; then
        print_success "All health checks passed!"
    else
        print_warning "Some checks failed - see above for details"
    fi
}

# Help command
cmd_help() {
    echo -e "${BOLD}${BLUE}ai-stack${NC} - Local AI Stack Management"
    echo ""
    echo -e "${BOLD}Usage:${NC}"
    echo "  ai-stack <command> [options]"
    echo ""
    echo -e "${BOLD}Commands:${NC}"
    echo -e "  ${CYAN}start${NC} [ollama|comfyui|all]   Start services (default: all)"
    echo -e "  ${CYAN}stop${NC} [ollama|comfyui|all]    Stop services (default: all)"
    echo -e "  ${CYAN}status${NC}                       Show running services, ports, memory"
    echo -e "  ${CYAN}models${NC}                       List Ollama models with sizes"
    echo -e "  ${CYAN}test${NC}                         Health check both services"
    echo -e "  ${CYAN}help${NC}                         Show this help message"
    echo ""
    echo -e "${BOLD}Examples:${NC}"
    echo "  ai-stack start              # Start all services"
    echo "  ai-stack start ollama       # Start only Ollama"
    echo "  ai-stack stop comfyui       # Stop only ComfyUI"
    echo "  ai-stack status             # Check what's running"
    echo "  ai-stack test               # Run health checks"
    echo ""
    echo -e "${BOLD}Services:${NC}"
    echo -e "  Ollama:   http://127.0.0.1:$OLLAMA_PORT"
    echo -e "  ComfyUI:  http://127.0.0.1:$COMFYUI_PORT"
}

# Main
main() {
    local command=${1:-help}
    shift 2>/dev/null || true

    case $command in
        start)
            cmd_start "$@"
            ;;
        stop)
            cmd_stop "$@"
            ;;
        status)
            cmd_status
            ;;
        models)
            cmd_models
            ;;
        test)
            cmd_test
            ;;
        help|--help|-h)
            cmd_help
            ;;
        *)
            print_error "Unknown command: $command"
            echo ""
            cmd_help
            exit 1
            ;;
    esac
}

main "$@"
